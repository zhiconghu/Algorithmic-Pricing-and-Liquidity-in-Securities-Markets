{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# Library for multi-threading\n",
    "from multiprocessing import Process, current_process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "_episodes_ (int): Number of episodes in experiment\n",
    "_seed_ (int): Seed for random generation\n",
    "\n",
    "Result: An array with length (_episodes_) filled with either _payoff_low_/_payoff_high_\n",
    "\"\"\"\n",
    "\n",
    "def generate_asset_payoff(_prob_low_, _prob_high_, _payoff_low_, _payoff_high_, _episodes_, _seed_):\n",
    "\n",
    "    # Generate an array of random asset payoff\n",
    "    random.seed(_seed_)\n",
    "    asset_payoff = random.uniform(0,1,_episodes_)\n",
    "    asset_payoff[asset_payoff < _prob_low_] = _payoff_low_\n",
    "    asset_payoff[asset_payoff >= _prob_high_] = _payoff_high_\n",
    "\n",
    "    return asset_payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "_asset_payoff_ (array): Array of realized asset payoff values\n",
    "_private_valuation_mean_ (float): Mean of private valuation of traders\n",
    "_private_valuation_sd_ (float): Standard deviation of private valuation of traders\n",
    "_seed_ (int): Seed for random generation\n",
    "\n",
    "Result: An array with the same length as the (_asset_payoff_) parameter, with trader's private valuation\n",
    "\"\"\"\n",
    "\n",
    "def generate_trader_valuation(_asset_payoff_, _private_valuation_mean_, _private_valuation_sd_, _seed_):\n",
    "\n",
    "    # Generate an array for the valuation of the asset of continuous trader\n",
    "    random.seed(_seed_)\n",
    "    trader_valuation = _asset_payoff_ + random.normal(_private_valuation_mean_, _private_valuation_sd_, len(_asset_payoff_))\n",
    "\n",
    "    return(trader_valuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "_episodes_ (int): Number of episodes in experiment\n",
    "_seed_ (int): Seed for random generation\n",
    "\n",
    "Result: An array of length (_episodes_) filled with \"explore\"/\"exploit\"\n",
    "\"\"\"\n",
    "\n",
    "def generate_dealer_action(_beta_, _episodes_, _seed_):\n",
    "\n",
    "    # Generate a series of dealer action with probability of \"explore\" decreasing with the function exp(-beta*t)\n",
    "    func = np.vectorize(lambda t: np.exp(-_beta_*t))\n",
    "    dealer_action_prob = func(np.arange(0,_episodes_,1))\n",
    "\n",
    "    random.seed(_seed_)\n",
    "    dealer_action = random.uniform(0, 1, _episodes_)\n",
    "    dealer_action = (dealer_action_prob >= dealer_action).astype(int).astype(str)\n",
    "    dealer_action[dealer_action == '1'] = \"explore\"\n",
    "    dealer_action[dealer_action == '0'] = \"exploit\"\n",
    "\n",
    "    return dealer_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "_lower_q_ (int): Lower limit of the uniform distribution to select initial values from\n",
    "_upper_q_ (int): Upper limit of the uniform distribution to select initial values from\n",
    "_no_possible_prices_ (int): Possible ask prices by the dealer\n",
    "_no_dealers_ (int): Number of dealers in the environment\n",
    "_seed_ (int): Seed for random generation\n",
    "\n",
    "Result: Matrix of dimensions (_no_possible_prices_ x _no_dealers_)\n",
    "\"\"\"\n",
    "\n",
    "def generate_q_matrix(_lower_q_, _upper_q_, _no_possible_prices_, _no_dealers_, _seed_):\n",
    "\n",
    "    # Generate Q_matrix that indicates the expected payoff of the dealer asking each price\n",
    "    random.seed(_seed_)\n",
    "    Q_matrix = pd.DataFrame(np.random.uniform(_lower_q_, _upper_q_, size = (_no_possible_prices_,_no_dealers_)), columns = [\"dealer_\" + x for x in list(map(str, np.arange(_no_dealers_)+1))])\n",
    "    \n",
    "    return(Q_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asset parameters\n",
    "payoff_high = 4\n",
    "prob_high = 0.5\n",
    "payoff_low = 0\n",
    "prob_low = 0.5\n",
    "expected_payoff = payoff_high*prob_high + payoff_low*prob_low\n",
    "\n",
    "# Dealer parameter\n",
    "no_of_dealer = 1\n",
    "lowest_ask_price = 1\n",
    "highest_ask_price = 15\n",
    "possible_ask_price = np.arange(lowest_ask_price, highest_ask_price+1, 1)\n",
    "\n",
    "# Trader parameters\n",
    "private_valuation_mean = 0\n",
    "private_valuation_sd = 0.5\n",
    "\n",
    "# Learning parameters\n",
    "alpha = 0.01\n",
    "beta = 0.0008\n",
    "lower_q = 3\n",
    "upper_q = 6\n",
    "\n",
    "# Number of Experiments and Episode\n",
    "K = 100\n",
    "T = 200000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try the \"generate_asset_payoff\" function with our above parameters. This should generate an array of 0 or 4 values each with a probability of 1/2 and the total length of the array is T episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4., 0., ..., 0., 4., 4.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"generate_trader_valuation\" function, this will generate an array of values of informed traders' valuation of the asset, each trader will have their own private valuation normally distributed with self-defined mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81217268,  3.69412179, -0.26408588, ...,  0.41681861,\n",
       "        4.00186687,  4.38609922])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_trader_valuation(generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,1), private_valuation_mean,private_valuation_sd,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"generate_dealer_action\" function, we generate an array of either \"explore\" or \"exploit\" with the probability of \"explore\" decreasing over the array. The probability of \"explore\" at episode t is defined by the equation exp(-beta*t). With 200,000 episodes, we expect an average of 1249.5 \"explore\" actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['exploit', 'explore'], dtype='<U21'), array([198777,   1223]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(generate_dealer_action(beta,T,1), return_counts = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"generate_q_matrix\" function, we generate a 1-dimensional array with values randomly sampled from a uniform distribution defined by U(lower_q, upper_q) and the total length of the array is the possible ask price the dealer can choose from. This is the initial Q-matrix of the dealer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dealer_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.251066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.160973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.906998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.440268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.277016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.558781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.036682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.190302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.616450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.257584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.055659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.613357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.634352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.082163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dealer_1\n",
       "0   4.251066\n",
       "1   5.160973\n",
       "2   3.000343\n",
       "3   3.906998\n",
       "4   3.440268\n",
       "5   3.277016\n",
       "6   3.558781\n",
       "7   4.036682\n",
       "8   4.190302\n",
       "9   4.616450\n",
       "10  4.257584\n",
       "11  5.055659\n",
       "12  3.613357\n",
       "13  5.634352\n",
       "14  3.082163"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_q_matrix(lower_q, upper_q, len(possible_ask_price), no_of_dealer,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Functions\n",
    "\n",
    "Here, we will write the functions for a single monopoly experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make sure to have all variables predefined before running the \"monopoly_experiment\" function\n",
    "\n",
    "Parameter:\n",
    "payoff_high (int): Value of asset when payoff is high\n",
    "prob_high (float): Probability the value of asset is high\n",
    "payoff_low (int): Value of asset when payoff is low\n",
    "prob_low (float): Probability the value of asset is low\n",
    "lowest_ask_price (int): Lowest ask price dealer can offer\n",
    "highest_ask_price (int): Highest ask price dealer can offer\n",
    "private_valuation_mean (float): Mean of private valuation of traders\n",
    "private_valuation_sd (float): Standard deviation of private valuation of traders\n",
    "alpha (float): Parameter determines how fast dealer learns from recent trades (higher means learn faster)\n",
    "beta (float): Parameter determines how often dealer chooses \"explore\" action (higher means less often)\n",
    "lower_q (int): Lower bound of initial Q_matrix\n",
    "upper_q (int): Upper bound of initial Q_matrix\n",
    "T (int): Maximum number of episode in each experiment\n",
    "k (int): Random seed for the experiment\n",
    "\n",
    "Results:\n",
    "1) An array of historical dealer price\n",
    "2) Summary of comparative statistics (trading volume, quoted spread, realized spread)\n",
    "\"\"\"\n",
    "\n",
    "def monopoly_experiment(payoff_high, prob_high, payoff_low, prob_low, # Asset parameters\n",
    "                        lowest_ask_price, highest_ask_price, # Dealer parameter\n",
    "                        private_valuation_mean, private_valuation_sd, # Trader parameters\n",
    "                        alpha, beta, lower_q, upper_q, T, # Learning parameters\n",
    "                        k # Random seed for experiment\n",
    "                        ):\n",
    "    \n",
    "    # Calculate initial information\n",
    "    possible_ask_price = np.arange(lowest_ask_price, highest_ask_price+1, 1)\n",
    "\n",
    "    # This array saves the historical greedy prices for each experiment\n",
    "    historical_greedy_price = np.array([])\n",
    "    # This array saves the historical trading volume for each experiment\n",
    "    historical_trading_volume = np.array([])\n",
    "    # This array saves the historical quoted spread for each experiment\n",
    "    historical_quoted_spread = np.array([])\n",
    "    # This array saves the historical realized spread for each experiment\n",
    "    historical_realized_spread = np.array([])\n",
    "\n",
    "    # Initate experiment with initial variables (Asset Payoff, Trader Valuation, Dealer Action, Q-Matrix)\n",
    "    asset_payoff = generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k)\n",
    "    trader_valuation = generate_trader_valuation(generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k), private_valuation_mean,private_valuation_sd,k)\n",
    "    dealer_action = generate_dealer_action(beta,T,k)\n",
    "    Q_matrix = generate_q_matrix(lower_q, upper_q, len(possible_ask_price), 1,k)\n",
    "\n",
    "    # Set random seed for ask_price randomisation during \"explore\", we iterate this over every experiment to make sure different experiments are selecting different \"explore\" ask prices\n",
    "    random.seed(k)\n",
    "\n",
    "    # Loop over each episode\n",
    "    for t in np.arange(0,T,1):\n",
    "        \n",
    "        # Dealer chooses to explore or exploit\n",
    "        if dealer_action[t] == \"explore\":\n",
    "            # If explore, ask_price is random integer from 1 to 15, since we have set seed=k above, this makes sure that ask_prices are taken at random differently in each experiment\n",
    "            ask_price = random.choice(possible_ask_price)\n",
    "\n",
    "        if dealer_action[t] == \"exploit\":\n",
    "            # If exploit, ask_price is the action with the highest expected payoff from Q_matrix\n",
    "            ask_price = np.argmax(Q_matrix.iloc[:,0])+1\n",
    "\n",
    "            # Save the historical ask prices and stop if greedy price didn't change for 10000 episodes\n",
    "            historical_greedy_price = np.append(historical_greedy_price, ask_price)\n",
    "            if len(historical_greedy_price) > 10000 and np.std(historical_greedy_price[-10000:]) == 0:\n",
    "                break       \n",
    "        \n",
    "        # Save the historical quoted spread\n",
    "        if t >= 1: historical_quoted_spread = np.append(historical_quoted_spread, ask_price-np.mean(asset_payoff[0:t]))\n",
    "\n",
    "        # Informed trader now chooses whether to trade in this episode according to the ask_price\n",
    "        # Case 1) Trader chooses to trade\n",
    "        if trader_valuation[t] >= ask_price:\n",
    "            # We update the Q-matrix of the dealer based on the profit made when a trade occurred in this episode\n",
    "            Q_matrix.iloc[ask_price-1,0] = alpha*(ask_price-asset_payoff[t]) + (1-alpha)*Q_matrix.iloc[ask_price-1,0]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,1)\n",
    "            # Save the historical realized spread\n",
    "            historical_realized_spread = np.append(historical_realized_spread, ask_price-asset_payoff[t])\n",
    "\n",
    "        # Case 2) Trader chooses not to trade\n",
    "        if trader_valuation[t] < ask_price:\n",
    "            # Otherwise, profit is 0 and the Q-matrix of the dealer is updated accordingly\n",
    "            Q_matrix.iloc[ask_price-1,0] = (1-alpha)*Q_matrix.iloc[ask_price-1,0]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,0)\n",
    "\n",
    "    # For tracking of progress, prints every 100 iteration of the experiment\n",
    "    if k%100 == 0: print(f\" Processor{current_process().name} is processing k={k}\")\n",
    "\n",
    "    # Return historical_greedy_price and comparative statistics\n",
    "    return historical_greedy_price, np.mean(historical_trading_volume), np.mean(historical_quoted_spread), np.mean(historical_realized_spread)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will write the functions for a duopoly experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make sure to have all variables predefined before running the \"duopoly_experiment\" function\n",
    "\n",
    "Parameter:\n",
    "payoff_high (int): Value of asset when payoff is high\n",
    "prob_high (float): Probability the value of asset is high\n",
    "payoff_low (int): Value of asset when payoff is low\n",
    "prob_low (float): Probability the value of asset is low\n",
    "lowest_ask_price (int): Lowest ask price dealer can offer\n",
    "highest_ask_price (int): Highest ask price dealer can offer\n",
    "private_valuation_mean (float): Mean of private valuation of traders\n",
    "private_valuation_sd (float): Standard deviation of private valuation of traders\n",
    "alpha (float): Parameter determines how fast dealer learns from recent trades (higher means learn faster)\n",
    "beta (float): Parameter determines how often dealer chooses \"explore\" action (higher means less often)\n",
    "lower_q (int): Lower bound of initial Q_matrix\n",
    "upper_q (int): Upper bound of initial Q_matrix\n",
    "T (int): Maximum number of episode in each experiment\n",
    "k (int): Random seed for the experiment\n",
    "\n",
    "Results:\n",
    "1) An array of historical dealer price\n",
    "2) Summary of comparative statistics (trading volume, quoted spread, realized spread)\n",
    "\"\"\"\n",
    "\n",
    "def duopoly_experiment(payoff_high, prob_high, payoff_low, prob_low, # Asset parameters\n",
    "                       lowest_ask_price, highest_ask_price, # Dealer parameter\n",
    "                       private_valuation_mean, private_valuation_sd, # Trader parameters\n",
    "                       alpha, beta, lower_q, upper_q, T, # Learning parameters\n",
    "                       k # Random seed for experiment\n",
    "                       ):\n",
    "    \n",
    "    # Calculate initial information\n",
    "    possible_ask_price = np.arange(lowest_ask_price, highest_ask_price+1, 1)\n",
    "    \n",
    "    # This array saves the historical greedy prices for each experiment\n",
    "    dealer_1_historical_greedy_price = np.array([])\n",
    "    dealer_2_historical_greedy_price = np.array([])\n",
    "    # This array saves the historical trading volume for each experiment\n",
    "    historical_trading_volume = np.array([])\n",
    "    # This array saves the historical quoted spread for each experiment\n",
    "    historical_quoted_spread = np.array([])\n",
    "    # This array saves the historical realized spread for each experiment\n",
    "    historical_realized_spread = np.array([])\n",
    "\n",
    "    # Initate experiment with initial variables (Asset Payoff, Trader Valuation, Dealer Action, Q-Matrix)\n",
    "    asset_payoff = generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k)\n",
    "    trader_valuation = generate_trader_valuation(generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k), private_valuation_mean,private_valuation_sd,k)\n",
    "    # For dealer_2, we added K to its initial seed so that two dealer's action in any experiment will not be the same, dealer_1's action will loop from 1 to K and dealer_2's action will loop from K+1 to 2K\n",
    "    dealer_1_action = generate_dealer_action(beta,T,k)\n",
    "    dealer_2_action = generate_dealer_action(beta,T,K+k)\n",
    "    Q_matrix = generate_q_matrix(lower_q, upper_q, len(possible_ask_price), 2,k)\n",
    "\n",
    "    # Set random seed for ask_price randomisation during \"explore\", we iterate this over every experiment to make sure different experiments are selecting different \"explore\" ask prices\n",
    "    random.seed(k)\n",
    "\n",
    "\n",
    "    # Loop over each episode\n",
    "    for t in np.arange(0,T,1):\n",
    "        \n",
    "        # Dealer 1 choose to explore or exploit\n",
    "        if dealer_1_action[t] == \"explore\":\n",
    "            # If explore, ask_price is random integer from 1 to 15, since we have set seed=k above, this makes sure that ask_prices are taken at random differently in each experiment\n",
    "            dealer_1_ask_price = random.choice(possible_ask_price)\n",
    "\n",
    "        if dealer_1_action[t] == \"exploit\":\n",
    "            # If exploit, ask_price is the action with the highest expected payoff from Q_matrix\n",
    "            dealer_1_ask_price = np.argmax(Q_matrix.iloc[:,0])+1\n",
    "\n",
    "            # Use an array to save historical ask prices\n",
    "            dealer_1_historical_greedy_price = np.append(dealer_1_historical_greedy_price, dealer_1_ask_price)\n",
    "\n",
    "            # Stop if greedy price for both dealers didn't change for 10000 episodes\n",
    "            if len(dealer_1_historical_greedy_price) > 10000 and np.std(dealer_1_historical_greedy_price[-10000:]) == 0 and \\\n",
    "            len(dealer_2_historical_greedy_price) > 10000 and np.std(dealer_2_historical_greedy_price[-10000:]) == 0:\n",
    "                break\n",
    "\n",
    "\n",
    "        # Dealer 2 choose to explore or exploit\n",
    "        if dealer_2_action[t] == \"explore\":\n",
    "            # If explore, ask_price is random integer from 1 to 15, since we have set seed=k above, this makes sure that ask_prices are taken at random differently in each experiment\n",
    "            dealer_2_ask_price = random.choice(possible_ask_price)\n",
    "\n",
    "        if dealer_2_action[t] == \"exploit\":\n",
    "            # If exploit, ask_price is the action with the highest expected payoff from Q_matrix\n",
    "            dealer_2_ask_price = np.argmax(Q_matrix.iloc[:,1])+1\n",
    "\n",
    "            # Use an array to save historical ask prices\n",
    "            dealer_2_historical_greedy_price = np.append(dealer_2_historical_greedy_price, dealer_2_ask_price)\n",
    "\n",
    "            # Stop if greedy price for both dealers didn't change for 10000 episodes\n",
    "            if len(dealer_1_historical_greedy_price) > 10000 and np.std(dealer_1_historical_greedy_price[-10000:]) == 0 and \\\n",
    "            len(dealer_2_historical_greedy_price) > 10000 and np.std(dealer_2_historical_greedy_price[-10000:]) == 0:\n",
    "                break\n",
    "\n",
    "\n",
    "        # Minimum dealer price for this episode\n",
    "        all_ask_prices = [dealer_1_ask_price, dealer_2_ask_price]\n",
    "        lower_ask_price = min(all_ask_prices)\n",
    "        higher_ask_price = max(all_ask_prices)\n",
    "\n",
    "        # Save the historical quoted spread\n",
    "        if t >= 1: historical_quoted_spread = np.append(historical_quoted_spread, lower_ask_price-np.mean(asset_payoff[0:t]))\n",
    "\n",
    "        # Create a binary variable that is 1 when both dealer offers the same ask price\n",
    "        if np.std(all_ask_prices) == 0: \n",
    "            whether_same_ask_prices = 1\n",
    "        else:\n",
    "            whether_same_ask_prices = 0\n",
    "\n",
    "\n",
    "        # Informed trader now chooses whether to trade in this episode according to the lowest_ask_price and which dealer to trade with\n",
    "        # Case 1) Trader chooses to trade with both dealers\n",
    "        if trader_valuation[t] >= lower_ask_price and whether_same_ask_prices == 1:\n",
    "            Q_matrix.iloc[dealer_1_ask_price-1,0] = alpha*(dealer_1_ask_price-asset_payoff[t])/2 + (1-alpha)*Q_matrix.iloc[dealer_1_ask_price-1,0]\n",
    "            Q_matrix.iloc[dealer_2_ask_price-1,1] = alpha*(dealer_2_ask_price-asset_payoff[t])/2 + (1-alpha)*Q_matrix.iloc[dealer_2_ask_price-1,1]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,1)\n",
    "            # Save the historical realized spread\n",
    "            historical_realized_spread = np.append(historical_realized_spread, lower_ask_price-asset_payoff[t])\n",
    "\n",
    "        # Case 2) Trader chooses to trade with one dealer with the lower ask price\n",
    "        if trader_valuation[t] >= lower_ask_price and whether_same_ask_prices == 0:\n",
    "            lower_price_dealer = np.argmin(all_ask_prices)\n",
    "            higher_price_dealer = np.argmax(all_ask_prices)\n",
    "            Q_matrix.iloc[lower_ask_price-1,lower_price_dealer] = alpha*(lower_ask_price-asset_payoff[t]) + (1-alpha)*Q_matrix.iloc[lower_ask_price-1,lower_price_dealer]\n",
    "            Q_matrix.iloc[higher_ask_price-1,higher_price_dealer] = (1-alpha)*Q_matrix.iloc[higher_ask_price-1,higher_price_dealer]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,1)\n",
    "            # Save the historical realized spread\n",
    "            historical_realized_spread = np.append(historical_realized_spread, lower_ask_price-asset_payoff[t])\n",
    "\n",
    "        # Case 3) Trader chooses not to trade\n",
    "        if trader_valuation[t] < lower_ask_price:\n",
    "            Q_matrix.iloc[dealer_1_ask_price-1,0] = (1-alpha)*Q_matrix.iloc[dealer_1_ask_price-1,0]\n",
    "            Q_matrix.iloc[dealer_2_ask_price-1,1] = (1-alpha)*Q_matrix.iloc[dealer_2_ask_price-1,1]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,0)\n",
    "\n",
    "         \n",
    "    # For tracking of progress, prints every 100 iteration of the experiment\n",
    "    if k%100 == 0: print(f\" Processor{current_process().name} is processing k={k}\")\n",
    "\n",
    "    # Return historical_greedy_price and comparative statistics\n",
    "    return dealer_1_historical_greedy_price, dealer_2_historical_greedy_price, np.mean(historical_trading_volume), np.mean(historical_quoted_spread), np.mean(historical_realized_spread)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will write the functions for a multi-agent experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make sure to have all variables predefined before running the \"multiagent_experiment\" function\n",
    "\n",
    "Parameter:\n",
    "payoff_high (int): Value of asset when payoff is high\n",
    "prob_high (float): Probability the value of asset is high\n",
    "payoff_low (int): Value of asset when payoff is low\n",
    "prob_low (float): Probability the value of asset is low\n",
    "no_dealers (int): Number of dealer in the experiment\n",
    "lowest_ask_price (int): Lowest ask price dealer can offer\n",
    "highest_ask_price (int): Highest ask price dealer can offer\n",
    "private_valuation_mean (float): Mean of private valuation of traders\n",
    "private_valuation_sd (float): Standard deviation of private valuation of traders\n",
    "alpha (float): Parameter determines how fast dealer learns from recent trades (higher means learn faster)\n",
    "beta (float): Parameter determines how often dealer chooses \"explore\" action (higher means less often)\n",
    "lower_q (int): Lower bound of initial Q_matrix\n",
    "upper_q (int): Upper bound of initial Q_matrix\n",
    "T (int): Maximum number of episode in each experiment\n",
    "k (int): Random seed for the experiment\n",
    "\n",
    "Results:\n",
    "1) An array of historical dealer price\n",
    "2) Summary of comparative statistics (trading volume, quoted spread, realized spread)\n",
    "\"\"\"\n",
    "\n",
    "def multiagent_experiment(payoff_high, prob_high, payoff_low, prob_low, # Asset parameters\n",
    "                          no_dealers, lowest_ask_price, highest_ask_price, # Dealer parameter\n",
    "                          private_valuation_mean, private_valuation_sd, # Trader parameters\n",
    "                          alpha, beta, lower_q, upper_q, T, # Learning parameters\n",
    "                          k # Random seed for experiment\n",
    "                          ):\n",
    "    \n",
    "    # Calculate initial information\n",
    "    possible_ask_price = np.arange(lowest_ask_price, highest_ask_price+1, 1)\n",
    "    \n",
    "    # This list saves the historical greedy prices in arrays for each experiment\n",
    "    historical_greedy_price = [np.array([]) for _ in np.arange(no_dealers)]\n",
    "    # This array saves the historical trading volume for each experiment\n",
    "    historical_trading_volume = np.array([])\n",
    "    # This array saves the historical quoted spread for each experiment\n",
    "    historical_quoted_spread = np.array([])\n",
    "    # This array saves the historical realized spread for each experiment\n",
    "    historical_realized_spread = np.array([])\n",
    "\n",
    "    # Initate experiment with initial variables (Asset Payoff, Trader Valuation, Dealer Action, Q-Matrix)\n",
    "    asset_payoff = generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k)\n",
    "    trader_valuation = generate_trader_valuation(generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k), private_valuation_mean,private_valuation_sd,k)\n",
    "    dealer_action = [generate_dealer_action(beta,T,1+n*K) for n in np.arange(1,no_dealers+1,1)]\n",
    "    Q_matrix = generate_q_matrix(lower_q, upper_q, len(possible_ask_price), no_dealers,k)\n",
    "\n",
    "    # Set random seed for ask_price randomisation during \"explore\", we iterate this over every experiment to make sure different experiments are selecting different \"explore\" ask prices\n",
    "    random.seed(k)\n",
    "\n",
    "    # Loop over each episode\n",
    "    for t in np.arange(0,T,1):\n",
    "\n",
    "        \"\"\"\n",
    "        Getting Ask Prices\n",
    "        \"\"\"\n",
    "        # We use an array to store ask prices of dealers\n",
    "        dealer_ask_prices_array = np.array([])\n",
    "\n",
    "        # Loop over all dealers and get ask price for each of them\n",
    "        for n in np.arange(no_dealers):\n",
    "\n",
    "            # Dealer n choose to explore or exploit\n",
    "            if dealer_action[n][t] == \"explore\":\n",
    "                # If explore, ask_price is random integer from 1 to 15, since we have set seed=k above, this makes sure that ask_prices are taken at random differently in each experiment\n",
    "                dealer_ask_prices_array = np.append(dealer_ask_prices_array, random.choice(possible_ask_price))\n",
    "\n",
    "            if dealer_action[n][t] == \"exploit\":\n",
    "                # If exploit, ask_price is the action with the highest expected payoff from the dealer's Q_matrix\n",
    "                dealer_ask_price = np.argmax(Q_matrix.iloc[:,n])+1\n",
    "                dealer_ask_prices_array = np.append(dealer_ask_prices_array, dealer_ask_price)\n",
    "\n",
    "                # Save historical greedy price for dealer n into the corresponding list\n",
    "                historical_greedy_price[n] = np.append(historical_greedy_price[n], dealer_ask_price)\n",
    "\n",
    "        # Stop if greedy price for all dealers didn't change for 10000 episodes\n",
    "        if min([len(array) for array in historical_greedy_price]) > 10000 and max([np.std(array[-10000:]) for array in historical_greedy_price]) == 0:\n",
    "            break\n",
    "\n",
    "        \"\"\"\n",
    "        Saving Comparative Stats\n",
    "        \"\"\"\n",
    "        # Minimum dealer price for this episode\n",
    "        lower_ask_price = min(dealer_ask_prices_array)\n",
    "\n",
    "        # Save the historical quoted spread\n",
    "        if t >= 1: historical_quoted_spread = np.append(historical_quoted_spread, lower_ask_price-np.mean(asset_payoff[0:t]))\n",
    "        # Save the historical trading volume and historical realized spread\n",
    "        if trader_valuation[t] >= lower_ask_price: \n",
    "            historical_trading_volume = np.append(historical_trading_volume,1)\n",
    "            historical_realized_spread = np.append(historical_realized_spread, lower_ask_price-asset_payoff[t])\n",
    "        else:\n",
    "            historical_trading_volume = np.append(historical_trading_volume,0)\n",
    "\n",
    "        \"\"\"\n",
    "        Updating Q-Matrix\n",
    "        \"\"\"\n",
    "        # Informed trader now chooses whether to trade in this episode according to the lowest_ask_price and how many dealer to trade with\n",
    "        num_of_dealer_to_trade = list(dealer_ask_prices_array).count(lower_ask_price)\n",
    "        # Again, we loop over the dealers to determine whether they gets the trade and how their Q-matrix is updated\n",
    "        for n in np.arange(no_dealers):\n",
    "\n",
    "            # Case 1) Trader n gets the trade\n",
    "            if trader_valuation[t] >= lower_ask_price and dealer_ask_prices_array[n] == lower_ask_price:\n",
    "                Q_matrix.iloc[int(lower_ask_price-1),n] = alpha*(lower_ask_price-asset_payoff[t])/num_of_dealer_to_trade + (1-alpha)*Q_matrix.iloc[int(lower_ask_price-1),n]\n",
    "\n",
    "            # Case 2) Trader n does not get the trade\n",
    "            else:\n",
    "                Q_matrix.iloc[int(dealer_ask_prices_array[n]-1),n] = (1-alpha)*Q_matrix.iloc[int(dealer_ask_prices_array[n]-1),n]\n",
    "    \n",
    "    # For tracking of progress, prints every 100 iteration of the experiment\n",
    "    if k%10 == 0: print(f\" Processor{current_process().name} is processing k={k}\")\n",
    "\n",
    "    # Return historical_greedy_price and comparative statistics\n",
    "    return historical_greedy_price, np.mean(historical_trading_volume), np.mean(historical_quoted_spread), np.mean(historical_realized_spread)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
