{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "\n",
    "# Library for multi-threading\n",
    "from multiprocessing import current_process, pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "_episodes_ (int): Number of episodes in experiment\n",
    "_seed_ (int): Seed for random generation\n",
    "\n",
    "Result: An array with length (_episodes_) filled with either _payoff_low_/_payoff_high_\n",
    "\"\"\"\n",
    "\n",
    "def generate_asset_payoff(_prob_low_, _prob_high_, _payoff_low_, _payoff_high_, _episodes_, _seed_):\n",
    "\n",
    "    # Generate an array of random asset payoff\n",
    "    random.seed(_seed_)\n",
    "    asset_payoff = random.uniform(0,1,_episodes_)\n",
    "    asset_payoff[asset_payoff < _prob_low_] = _payoff_low_\n",
    "    asset_payoff[asset_payoff >= _prob_high_] = _payoff_high_\n",
    "\n",
    "    return asset_payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "_asset_payoff_ (array): Array of realized asset payoff values\n",
    "_private_valuation_mean_ (float): Mean of private valuation of traders\n",
    "_private_valuation_sd_ (float): Standard deviation of private valuation of traders\n",
    "_seed_ (int): Seed for random generation\n",
    "\n",
    "Result: An array with the same length as the (_asset_payoff_) parameter, with trader's private valuation\n",
    "\"\"\"\n",
    "\n",
    "def generate_trader_valuation(_asset_payoff_, _private_valuation_mean_, _private_valuation_sd_, _seed_):\n",
    "\n",
    "    # Generate an array for the valuation of the asset of continuous trader\n",
    "    random.seed(_seed_)\n",
    "    trader_valuation = _asset_payoff_ + random.normal(_private_valuation_mean_, _private_valuation_sd_, len(_asset_payoff_))\n",
    "\n",
    "    return(trader_valuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "_episodes_ (int): Number of episodes in experiment\n",
    "_seed_ (int): Seed for random generation\n",
    "\n",
    "Result: An array of length (_episodes_) filled with \"explore\"/\"exploit\"\n",
    "\"\"\"\n",
    "\n",
    "def generate_dealer_action(_beta_, _episodes_, _seed_):\n",
    "\n",
    "    # Generate a series of dealer action with probability of \"explore\" decreasing with the function exp(-beta*t)\n",
    "    func = np.vectorize(lambda t: np.exp(-_beta_*t))\n",
    "    dealer_action_prob = func(np.arange(0,_episodes_,1))\n",
    "\n",
    "    random.seed(_seed_)\n",
    "    dealer_action = random.uniform(0, 1, _episodes_)\n",
    "    dealer_action = (dealer_action_prob >= dealer_action).astype(int).astype(str)\n",
    "    dealer_action[dealer_action == '1'] = \"explore\"\n",
    "    dealer_action[dealer_action == '0'] = \"exploit\"\n",
    "\n",
    "    return dealer_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "_lower_q_ (int): Lower limit of the uniform distribution to select initial values from\n",
    "_upper_q_ (int): Upper limit of the uniform distribution to select initial values from\n",
    "_no_possible_prices_ (int): Possible ask prices by the dealer\n",
    "_no_dealers_ (int): Number of dealers in the environment\n",
    "_seed_ (int): Seed for random generation\n",
    "\n",
    "Result: Matrix of dimensions (_no_possible_prices_ x _no_dealers_)\n",
    "\"\"\"\n",
    "\n",
    "def generate_q_matrix(_lower_q_, _upper_q_, _no_possible_prices_, _no_dealers_, _seed_):\n",
    "\n",
    "    # Generate Q_matrix that indicates the expected payoff of the dealer asking each price\n",
    "    random.seed(_seed_)\n",
    "    Q_matrix = pd.DataFrame(np.random.uniform(_lower_q_, _upper_q_, size = (_no_possible_prices_,_no_dealers_)), columns = [\"dealer_\" + x for x in list(map(str, np.arange(_no_dealers_)+1))])\n",
    "    \n",
    "    return(Q_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asset parameters\n",
    "payoff_high = 4\n",
    "prob_high = 0.5\n",
    "payoff_low = 0\n",
    "prob_low = 0.5\n",
    "expected_payoff = payoff_high*prob_high + payoff_low*prob_low\n",
    "\n",
    "# Dealer parameter\n",
    "no_dealers = 2\n",
    "lowest_ask_price = 1\n",
    "highest_ask_price = 15\n",
    "possible_ask_price = np.arange(lowest_ask_price, highest_ask_price+1, 1)\n",
    "\n",
    "# Trader parameters\n",
    "private_valuation_mean = 0\n",
    "private_valuation_sd = 5\n",
    "\n",
    "# Learning parameters\n",
    "alpha = 0.01\n",
    "beta = 0.0008\n",
    "lower_q = 3\n",
    "upper_q = 6\n",
    "\n",
    "# Number of Experiments and Episode\n",
    "K = 100\n",
    "T = 200000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try the \"generate_asset_payoff\" function with our above parameters. This should generate an array of 0 or 4 values each with a probability of 1/2 and the total length of the array is T episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"generate_trader_valuation\" function, this will generate an array of values of informed traders' valuation of the asset, each trader will have their own private valuation normally distributed with self-defined mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_trader_valuation(generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,1), private_valuation_mean,private_valuation_sd,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"generate_dealer_action\" function, we generate an array of either \"explore\" or \"exploit\" with the probability of \"explore\" decreasing over the array. The probability of \"explore\" at episode t is defined by the equation exp(-beta*t). With 200,000 episodes, we expect an average of 1249.5 \"explore\" actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(generate_dealer_action(beta,T,1), return_counts = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the \"generate_q_matrix\" function, we generate a 1-dimensional array with values randomly sampled from a uniform distribution defined by U(lower_q, upper_q) and the total length of the array is the possible ask price the dealer can choose from. This is the initial Q-matrix of the dealer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_q_matrix(lower_q, upper_q, len(possible_ask_price), no_dealers,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Functions\n",
    "\n",
    "Here, we will write the functions for a single monopoly experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make sure to have all variables predefined before running the \"monopoly_experiment\" function\n",
    "\n",
    "Parameter:\n",
    "payoff_high (int): Value of asset when payoff is high\n",
    "prob_high (float): Probability the value of asset is high\n",
    "payoff_low (int): Value of asset when payoff is low\n",
    "prob_low (float): Probability the value of asset is low\n",
    "lowest_ask_price (int): Lowest ask price dealer can offer\n",
    "highest_ask_price (int): Highest ask price dealer can offer\n",
    "private_valuation_mean (float): Mean of private valuation of traders\n",
    "private_valuation_sd (float): Standard deviation of private valuation of traders\n",
    "alpha (float): Parameter determines how fast dealer learns from recent trades (higher means learn faster)\n",
    "beta (float): Parameter determines how often dealer chooses \"explore\" action (higher means less often)\n",
    "lower_q (int): Lower bound of initial Q_matrix\n",
    "upper_q (int): Upper bound of initial Q_matrix\n",
    "T (int): Maximum number of episode in each experiment\n",
    "k (int): Random seed for the experiment\n",
    "\n",
    "Results:\n",
    "1) An array of historical dealer price\n",
    "2) Summary of comparative statistics (trading volume, quoted spread, realized spread)\n",
    "\"\"\"\n",
    "\n",
    "def monopoly_experiment(payoff_high, prob_high, payoff_low, prob_low, # Asset parameters\n",
    "                        lowest_ask_price, highest_ask_price, # Dealer parameter\n",
    "                        private_valuation_mean, private_valuation_sd, # Trader parameters\n",
    "                        alpha, beta, lower_q, upper_q, T, # Learning parameters\n",
    "                        k # Random seed for experiment\n",
    "                        ):\n",
    "    \n",
    "    # Calculate initial information\n",
    "    possible_ask_price = np.arange(lowest_ask_price, highest_ask_price+1, 1)\n",
    "\n",
    "    # This array saves the historical greedy prices for each experiment\n",
    "    historical_greedy_price = np.array([])\n",
    "    # This array saves the historical trading volume for each experiment\n",
    "    historical_trading_volume = np.array([])\n",
    "    # This array saves the historical quoted spread for each experiment\n",
    "    historical_quoted_spread = np.array([])\n",
    "    # This array saves the historical realized spread for each experiment\n",
    "    historical_realized_spread = np.array([])\n",
    "\n",
    "    # Initate experiment with initial variables (Asset Payoff, Trader Valuation, Dealer Action, Q-Matrix)\n",
    "    asset_payoff = generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k)\n",
    "    trader_valuation = generate_trader_valuation(asset_payoff, private_valuation_mean,private_valuation_sd,k)\n",
    "    dealer_action = generate_dealer_action(beta,T,k)\n",
    "    Q_matrix = generate_q_matrix(lower_q, upper_q, len(possible_ask_price), 1,k)\n",
    "\n",
    "    # Set random seed for ask_price randomisation during \"explore\", we iterate this over every experiment to make sure different experiments are selecting different \"explore\" ask prices\n",
    "    random.seed(k)\n",
    "\n",
    "    # Loop over each episode\n",
    "    for t in np.arange(0,T,1):\n",
    "        \n",
    "        # Dealer chooses to explore or exploit\n",
    "        if dealer_action[t] == \"explore\":\n",
    "            # If explore, ask_price is random integer from 1 to 15, since we have set seed=k above, this makes sure that ask_prices are taken at random differently in each experiment\n",
    "            ask_price = random.choice(possible_ask_price)\n",
    "\n",
    "        if dealer_action[t] == \"exploit\":\n",
    "            # If exploit, ask_price is the action with the highest expected payoff from Q_matrix\n",
    "            ask_price = np.argmax(Q_matrix.iloc[:,0])+1\n",
    "\n",
    "            # Save the historical ask prices and stop if greedy price didn't change for 10000 episodes\n",
    "            historical_greedy_price = np.append(historical_greedy_price, ask_price)\n",
    "            if len(historical_greedy_price) > 10000 and np.std(historical_greedy_price[-10000:]) == 0:\n",
    "                break       \n",
    "        \n",
    "        # Save the historical quoted spread\n",
    "        if t >= 1: historical_quoted_spread = np.append(historical_quoted_spread, ask_price-np.mean(asset_payoff[0:t]))\n",
    "\n",
    "        # Informed trader now chooses whether to trade in this episode according to the ask_price\n",
    "        # Case 1) Trader chooses to trade\n",
    "        if trader_valuation[t] >= ask_price:\n",
    "            # We update the Q-matrix of the dealer based on the profit made when a trade occurred in this episode\n",
    "            Q_matrix.iloc[ask_price-1,0] = alpha*(ask_price-asset_payoff[t]) + (1-alpha)*Q_matrix.iloc[ask_price-1,0]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,1)\n",
    "            # Save the historical realized spread\n",
    "            historical_realized_spread = np.append(historical_realized_spread, ask_price-asset_payoff[t])\n",
    "\n",
    "        # Case 2) Trader chooses not to trade\n",
    "        if trader_valuation[t] < ask_price:\n",
    "            # Otherwise, profit is 0 and the Q-matrix of the dealer is updated accordingly\n",
    "            Q_matrix.iloc[ask_price-1,0] = (1-alpha)*Q_matrix.iloc[ask_price-1,0]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,0)\n",
    "\n",
    "    # For tracking of progress, prints every 100 iteration of the experiment\n",
    "    if k%100 == 0: print(f\" Processor{current_process().name} is processing k={k}\")\n",
    "\n",
    "    # Return historical_greedy_price and comparative statistics\n",
    "    return historical_greedy_price, np.mean(historical_trading_volume), np.mean(historical_quoted_spread), np.mean(historical_realized_spread)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will write the functions for a duopoly experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make sure to have all variables predefined before running the \"duopoly_experiment\" function\n",
    "\n",
    "Parameter:\n",
    "payoff_high (int): Value of asset when payoff is high\n",
    "prob_high (float): Probability the value of asset is high\n",
    "payoff_low (int): Value of asset when payoff is low\n",
    "prob_low (float): Probability the value of asset is low\n",
    "lowest_ask_price (int): Lowest ask price dealer can offer\n",
    "highest_ask_price (int): Highest ask price dealer can offer\n",
    "private_valuation_mean (float): Mean of private valuation of traders\n",
    "private_valuation_sd (float): Standard deviation of private valuation of traders\n",
    "alpha (float): Parameter determines how fast dealer learns from recent trades (higher means learn faster)\n",
    "beta (float): Parameter determines how often dealer chooses \"explore\" action (higher means less often)\n",
    "lower_q (int): Lower bound of initial Q_matrix\n",
    "upper_q (int): Upper bound of initial Q_matrix\n",
    "T (int): Maximum number of episode in each experiment\n",
    "k (int): Random seed for the experiment\n",
    "\n",
    "Results:\n",
    "1) An array of historical dealer price\n",
    "2) Summary of comparative statistics (trading volume, quoted spread, realized spread)\n",
    "\"\"\"\n",
    "\n",
    "def duopoly_experiment(payoff_high, prob_high, payoff_low, prob_low, # Asset parameters\n",
    "                       lowest_ask_price, highest_ask_price, # Dealer parameter\n",
    "                       private_valuation_mean, private_valuation_sd, # Trader parameters\n",
    "                       alpha, beta, lower_q, upper_q, T, # Learning parameters\n",
    "                       k # Random seed for experiment\n",
    "                       ):\n",
    "    \n",
    "    # Calculate initial information\n",
    "    possible_ask_price = np.arange(lowest_ask_price, highest_ask_price+1, 1)\n",
    "    \n",
    "    # This array saves the historical greedy prices for each experiment\n",
    "    dealer_1_historical_greedy_price = np.array([])\n",
    "    dealer_2_historical_greedy_price = np.array([])\n",
    "    # This array saves the historical trading volume for each experiment\n",
    "    historical_trading_volume = np.array([])\n",
    "    # This array saves the historical quoted spread for each experiment\n",
    "    historical_quoted_spread = np.array([])\n",
    "    # This array saves the historical realized spread for each experiment\n",
    "    historical_realized_spread = np.array([])\n",
    "\n",
    "    # Initate experiment with initial variables (Asset Payoff, Trader Valuation, Dealer Action, Q-Matrix)\n",
    "    asset_payoff = generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k)\n",
    "    trader_valuation = generate_trader_valuation(asset_payoff, private_valuation_mean,private_valuation_sd,k)\n",
    "    # For dealer_2, we added K to its initial seed so that two dealer's action in any experiment will not be the same, dealer_1's action will loop from 1 to K and dealer_2's action will loop from K+1 to 2K\n",
    "    dealer_1_action = generate_dealer_action(beta,T,k)\n",
    "    dealer_2_action = generate_dealer_action(beta,T,K+k)\n",
    "    Q_matrix = generate_q_matrix(lower_q, upper_q, len(possible_ask_price), 2,k)\n",
    "\n",
    "    # Set random seed for ask_price randomisation during \"explore\", we iterate this over every experiment to make sure different experiments are selecting different \"explore\" ask prices\n",
    "    random.seed(k)\n",
    "\n",
    "\n",
    "    # Loop over each episode\n",
    "    for t in np.arange(0,T,1):\n",
    "        \n",
    "        # Dealer 1 choose to explore or exploit\n",
    "        if dealer_1_action[t] == \"explore\":\n",
    "            # If explore, ask_price is random integer from 1 to 15, since we have set seed=k above, this makes sure that ask_prices are taken at random differently in each experiment\n",
    "            dealer_1_ask_price = random.choice(possible_ask_price)\n",
    "\n",
    "        if dealer_1_action[t] == \"exploit\":\n",
    "            # If exploit, ask_price is the action with the highest expected payoff from Q_matrix\n",
    "            dealer_1_ask_price = np.argmax(Q_matrix.iloc[:,0])+1\n",
    "\n",
    "            # Use an array to save historical ask prices\n",
    "            dealer_1_historical_greedy_price = np.append(dealer_1_historical_greedy_price, dealer_1_ask_price)\n",
    "\n",
    "            # Stop if greedy price for both dealers didn't change for 10000 episodes\n",
    "            if len(dealer_1_historical_greedy_price) > 10000 and np.std(dealer_1_historical_greedy_price[-10000:]) == 0 and \\\n",
    "            len(dealer_2_historical_greedy_price) > 10000 and np.std(dealer_2_historical_greedy_price[-10000:]) == 0:\n",
    "                break\n",
    "\n",
    "\n",
    "        # Dealer 2 choose to explore or exploit\n",
    "        if dealer_2_action[t] == \"explore\":\n",
    "            # If explore, ask_price is random integer from 1 to 15, since we have set seed=k above, this makes sure that ask_prices are taken at random differently in each experiment\n",
    "            dealer_2_ask_price = random.choice(possible_ask_price)\n",
    "\n",
    "        if dealer_2_action[t] == \"exploit\":\n",
    "            # If exploit, ask_price is the action with the highest expected payoff from Q_matrix\n",
    "            dealer_2_ask_price = np.argmax(Q_matrix.iloc[:,1])+1\n",
    "\n",
    "            # Use an array to save historical ask prices\n",
    "            dealer_2_historical_greedy_price = np.append(dealer_2_historical_greedy_price, dealer_2_ask_price)\n",
    "\n",
    "            # Stop if greedy price for both dealers didn't change for 10000 episodes\n",
    "            if len(dealer_1_historical_greedy_price) > 10000 and np.std(dealer_1_historical_greedy_price[-10000:]) == 0 and \\\n",
    "            len(dealer_2_historical_greedy_price) > 10000 and np.std(dealer_2_historical_greedy_price[-10000:]) == 0:\n",
    "                break\n",
    "\n",
    "\n",
    "        # Minimum dealer price for this episode\n",
    "        all_ask_prices = [dealer_1_ask_price, dealer_2_ask_price]\n",
    "        lower_ask_price = min(all_ask_prices)\n",
    "        higher_ask_price = max(all_ask_prices)\n",
    "\n",
    "        # Save the historical quoted spread\n",
    "        if t >= 1: historical_quoted_spread = np.append(historical_quoted_spread, lower_ask_price-np.mean(asset_payoff[0:t]))\n",
    "\n",
    "        # Create a binary variable that is 1 when both dealer offers the same ask price\n",
    "        if np.std(all_ask_prices) == 0: \n",
    "            whether_same_ask_prices = 1\n",
    "        else:\n",
    "            whether_same_ask_prices = 0\n",
    "\n",
    "\n",
    "        # Informed trader now chooses whether to trade in this episode according to the lowest_ask_price and which dealer to trade with\n",
    "        # Case 1) Trader chooses to trade with both dealers\n",
    "        if trader_valuation[t] >= lower_ask_price and whether_same_ask_prices == 1:\n",
    "            Q_matrix.iloc[dealer_1_ask_price-1,0] = alpha*(dealer_1_ask_price-asset_payoff[t])/2 + (1-alpha)*Q_matrix.iloc[dealer_1_ask_price-1,0]\n",
    "            Q_matrix.iloc[dealer_2_ask_price-1,1] = alpha*(dealer_2_ask_price-asset_payoff[t])/2 + (1-alpha)*Q_matrix.iloc[dealer_2_ask_price-1,1]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,1)\n",
    "            # Save the historical realized spread\n",
    "            historical_realized_spread = np.append(historical_realized_spread, lower_ask_price-asset_payoff[t])\n",
    "\n",
    "        # Case 2) Trader chooses to trade with one dealer with the lower ask price\n",
    "        if trader_valuation[t] >= lower_ask_price and whether_same_ask_prices == 0:\n",
    "            lower_price_dealer = np.argmin(all_ask_prices)\n",
    "            higher_price_dealer = np.argmax(all_ask_prices)\n",
    "            Q_matrix.iloc[lower_ask_price-1,lower_price_dealer] = alpha*(lower_ask_price-asset_payoff[t]) + (1-alpha)*Q_matrix.iloc[lower_ask_price-1,lower_price_dealer]\n",
    "            Q_matrix.iloc[higher_ask_price-1,higher_price_dealer] = (1-alpha)*Q_matrix.iloc[higher_ask_price-1,higher_price_dealer]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,1)\n",
    "            # Save the historical realized spread\n",
    "            historical_realized_spread = np.append(historical_realized_spread, lower_ask_price-asset_payoff[t])\n",
    "\n",
    "        # Case 3) Trader chooses not to trade\n",
    "        if trader_valuation[t] < lower_ask_price:\n",
    "            Q_matrix.iloc[dealer_1_ask_price-1,0] = (1-alpha)*Q_matrix.iloc[dealer_1_ask_price-1,0]\n",
    "            Q_matrix.iloc[dealer_2_ask_price-1,1] = (1-alpha)*Q_matrix.iloc[dealer_2_ask_price-1,1]\n",
    "\n",
    "            # Save the historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,0)\n",
    "\n",
    "         \n",
    "    # For tracking of progress, prints every 100 iteration of the experiment\n",
    "    if k%100 == 0: print(f\" Processor{current_process().name} is processing k={k}\")\n",
    "\n",
    "    # Return historical_greedy_price and comparative statistics\n",
    "    return dealer_1_historical_greedy_price, dealer_2_historical_greedy_price, np.mean(historical_trading_volume), np.mean(historical_quoted_spread), np.mean(historical_realized_spread)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will write the functions for a multi-agent experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make sure to have all variables predefined before running the \"multiagent_experiment\" function\n",
    "\n",
    "Parameter:\n",
    "payoff_high (int): Value of asset when payoff is high\n",
    "prob_high (float): Probability the value of asset is high\n",
    "payoff_low (int): Value of asset when payoff is low\n",
    "prob_low (float): Probability the value of asset is low\n",
    "no_dealers (int): Number of dealer in the experiment\n",
    "lowest_ask_price (int): Lowest ask price dealer can offer\n",
    "highest_ask_price (int): Highest ask price dealer can offer\n",
    "private_valuation_mean (float): Mean of private valuation of traders\n",
    "private_valuation_sd (float): Standard deviation of private valuation of traders\n",
    "alpha (float): Parameter determines how fast dealer learns from recent trades (higher means learn faster)\n",
    "beta (float): Parameter determines how often dealer chooses \"explore\" action (higher means less often)\n",
    "lower_q (int): Lower bound of initial Q_matrix\n",
    "upper_q (int): Upper bound of initial Q_matrix\n",
    "T (int): Maximum number of episode in each experiment\n",
    "k (int): Random seed for the experiment\n",
    "\n",
    "Results:\n",
    "1) An array of historical dealer price\n",
    "2) Summary of comparative statistics (trading volume, quoted spread, realized spread)\n",
    "\"\"\"\n",
    "\n",
    "def multiagent_experiment(payoff_high, prob_high, payoff_low, prob_low, # Asset parameters\n",
    "                          no_dealers, lowest_ask_price, highest_ask_price, # Dealer parameter\n",
    "                          private_valuation_mean, private_valuation_sd, # Trader parameters\n",
    "                          alpha, beta, lower_q, upper_q, T, # Learning parameters\n",
    "                          k # Random seed for experiment\n",
    "                          ):\n",
    "    \n",
    "    # Calculate initial information\n",
    "    possible_ask_price = np.arange(lowest_ask_price, highest_ask_price+1, 1)\n",
    "    \n",
    "    # This list saves the historical greedy prices in arrays for each experiment\n",
    "    historical_greedy_price = [np.array([]) for _ in np.arange(no_dealers)]\n",
    "    # This array saves the historical trading volume for each experiment\n",
    "    historical_trading_volume = np.array([])\n",
    "    # This array saves the historical quoted spread for each experiment\n",
    "    historical_quoted_spread = np.array([])\n",
    "    # This array saves the historical realized spread for each experiment\n",
    "    historical_realized_spread = np.array([])\n",
    "\n",
    "    # Initate experiment with initial variables (Asset Payoff, Trader Valuation, Dealer Action, Q-Matrix)\n",
    "    asset_payoff = generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k)\n",
    "    trader_valuation = generate_trader_valuation(asset_payoff, private_valuation_mean,private_valuation_sd,k)\n",
    "    dealer_action = [generate_dealer_action(beta,T,k+n*K) for n in np.arange(1,no_dealers+1,1)]\n",
    "    Q_matrix = generate_q_matrix(lower_q, upper_q, len(possible_ask_price), no_dealers,k)\n",
    "\n",
    "    # Set random seed for ask_price randomisation during \"explore\", we iterate this over every experiment to make sure different experiments are selecting different \"explore\" ask prices\n",
    "    random.seed(k)\n",
    "\n",
    "    # Loop over each episode\n",
    "    for t in np.arange(0,T,1):\n",
    "\n",
    "        \"\"\"\n",
    "        Getting Ask Prices\n",
    "        \"\"\"\n",
    "        # We use an array to store ask prices of dealers\n",
    "        dealer_ask_prices_array = np.array([])\n",
    "\n",
    "        # Loop over all dealers and get ask price for each of them\n",
    "        for n in np.arange(no_dealers):\n",
    "\n",
    "            # Dealer n choose to explore or exploit\n",
    "            if dealer_action[n][t] == \"explore\":\n",
    "                # If explore, ask_price is random integer from 1 to 15, since we have set seed=k above, this makes sure that ask_prices are taken at random differently in each experiment\n",
    "                dealer_ask_prices_array = np.append(dealer_ask_prices_array, random.choice(possible_ask_price))\n",
    "\n",
    "            if dealer_action[n][t] == \"exploit\":\n",
    "                # If exploit, ask_price is the action with the highest expected payoff from the dealer's Q_matrix\n",
    "                dealer_ask_price = np.argmax(Q_matrix.iloc[:,n])+1\n",
    "                dealer_ask_prices_array = np.append(dealer_ask_prices_array, dealer_ask_price)\n",
    "\n",
    "                # Save historical greedy price for dealer n into the corresponding list\n",
    "                historical_greedy_price[n] = np.append(historical_greedy_price[n], dealer_ask_price)\n",
    "\n",
    "        # Stop if greedy price for all dealers didn't change for 10000 episodes\n",
    "        if min([len(array) for array in historical_greedy_price]) > 10000 and max([np.std(array[-10000:]) for array in historical_greedy_price]) == 0:\n",
    "            break\n",
    "\n",
    "        \"\"\"\n",
    "        Saving Comparative Stats\n",
    "        \"\"\"\n",
    "        # Minimum dealer price for this episode\n",
    "        lowest_ask_price = min(dealer_ask_prices_array)\n",
    "\n",
    "        # Save the historical quoted spread\n",
    "        if t >= 1: historical_quoted_spread = np.append(historical_quoted_spread, lowest_ask_price-np.mean(asset_payoff[0:t]))\n",
    "\n",
    "        \"\"\"\n",
    "        Updating Q-Matrix\n",
    "        \"\"\"\n",
    "        # Informed trader now chooses whether to trade in this episode according to the lowest_ask_price\n",
    "        if trader_valuation[t] >= lowest_ask_price: \n",
    "\n",
    "            # Save the historical trading volume and historical realized spread\n",
    "            historical_trading_volume = np.append(historical_trading_volume,1)\n",
    "            historical_realized_spread = np.append(historical_realized_spread, lowest_ask_price-asset_payoff[t])\n",
    "\n",
    "            # Get the number of dealers to trade with\n",
    "            num_of_dealer_to_trade = list(dealer_ask_prices_array).count(lowest_ask_price)\n",
    "\n",
    "            # Now, we loop over the dealers to determine whether they gets the trade and how their Q-matrix is updated\n",
    "            for n in np.arange(no_dealers):\n",
    "\n",
    "                # Case 1) Dealer n gets the trade\n",
    "                if dealer_ask_prices_array[n] == lowest_ask_price:\n",
    "                    Q_matrix.iloc[int(lowest_ask_price-1),n] = alpha*(lowest_ask_price-asset_payoff[t])/num_of_dealer_to_trade + (1-alpha)*Q_matrix.iloc[int(lowest_ask_price-1),n]\n",
    "\n",
    "                # Case 2) Dealer n does not get the trade\n",
    "                else:\n",
    "                    Q_matrix.iloc[int(dealer_ask_prices_array[n]-1),n] = (1-alpha)*Q_matrix.iloc[int(dealer_ask_prices_array[n]-1),n]\n",
    "            \n",
    "        # If no trade occurs in this episode\n",
    "        else:\n",
    "            # Save historical trading volume\n",
    "            historical_trading_volume = np.append(historical_trading_volume,0)\n",
    "\n",
    "            # We update all of the dealers Q-matrix\n",
    "            for n in np.arange(no_dealers):\n",
    "                Q_matrix.iloc[int(dealer_ask_prices_array[n]-1),n] = (1-alpha)*Q_matrix.iloc[int(dealer_ask_prices_array[n]-1),n]\n",
    "\n",
    "    \n",
    "    # For tracking of progress, prints every 10 iteration of the experiment\n",
    "    if k%100 == 0: print(f\" Processor{current_process().name} is processing k={k}\")\n",
    "\n",
    "    # Return historical_greedy_price and comparative statistics\n",
    "    return historical_greedy_price, np.mean(historical_trading_volume), np.mean(historical_quoted_spread), np.mean(historical_realized_spread)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Discovery\n",
    "\n",
    "In this section, we will modify the above code for price discovery, each episode will have 2 trading periods.\n",
    "\n",
    "Before, writing the experiment, because the Q-matrix is this experiment has the shape M x (N+3), we need to write our \"generate_q_matrix\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "_lower_q_ (int): Lower limit of the uniform distribution to select initial values from\n",
    "_upper_q_ (int): Upper limit of the uniform distribution to select initial values from\n",
    "_no_possible_prices_ (int): Possible ask prices by the dealer\n",
    "_no_dealers_ (int): Number of dealers in the environment\n",
    "_seed_ (int): Seed for random generation\n",
    "\n",
    "Result:\n",
    "Nested list with layers being:\n",
    "1) Dealer number\n",
    "2) State (N+3 possiblities)\n",
    "3) Ask prices\n",
    "\n",
    "For Example, list[1][0][12] is the expected payoff of dealer 2 in state NT offering the ask price of 13.\n",
    "\"\"\"\n",
    "\n",
    "def generate_discovery_q_matrix(_lower_q_, _upper_q_, _no_possible_prices_, _no_dealers_, _seed_):\n",
    "\n",
    "    # Generate Q_matrix that indicates the expected payoff of the dealer asking each price\n",
    "    random.seed(_seed_)\n",
    "    Q_matrix = [np.random.uniform(_lower_q_, _upper_q_, size = (_no_dealers_+3,_no_possible_prices_)) for _ in np.arange(_no_dealers_)]\n",
    "    \n",
    "    return(Q_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function for 2 dealers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_discovery_q_matrix(lower_q,upper_q,len(possible_ask_price),2,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our Q-matrix, we have 2 dealers, for each dealer, the first row is the state null, which is the state in period 1. Following, the remaining rows are states in period 2. \n",
    "\n",
    "In our example, for each trader, the second column is when there are no trades in period 1, third column is when there is a trade but the dealer did not get to trade in period 1, forth column is when there is a trade and the dealer get 1/2 the trade in period 1 and the last column is when there the dealer gets the full trade in period 1.\n",
    "\n",
    "Now, we can write the experiment function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make sure to have all variables predefined before running the \"pricediscovery_experiment\" function\n",
    "\n",
    "Parameter:\n",
    "payoff_high (int): Value of asset when payoff is high\n",
    "prob_high (float): Probability the value of asset is high\n",
    "payoff_low (int): Value of asset when payoff is low\n",
    "prob_low (float): Probability the value of asset is low\n",
    "no_dealers (int): Number of dealer in the experiment\n",
    "lowest_ask_price (int): Lowest ask price dealer can offer\n",
    "highest_ask_price (int): Highest ask price dealer can offer\n",
    "private_valuation_mean (float): Mean of private valuation of traders\n",
    "private_valuation_sd (float): Standard deviation of private valuation of traders\n",
    "alpha (float): Parameter determines how fast dealer learns from recent trades (higher means learn faster)\n",
    "beta (float): Parameter determines how often dealer chooses \"explore\" action (higher means less often)\n",
    "lower_q (int): Lower bound of initial Q_matrix\n",
    "upper_q (int): Upper bound of initial Q_matrix\n",
    "T (int): Maximum number of episode in each experiment\n",
    "k (int): Random seed for the experiment\n",
    "\n",
    "Results:\n",
    "1) An array of historical dealer price\n",
    "2) Summary of trading volume, quoted spread, realized spread\n",
    "\"\"\"\n",
    "\n",
    "def pricediscovery_experiment(payoff_high, prob_high, payoff_low, prob_low, # Asset parameters\n",
    "                              no_dealers, lowest_ask_price, highest_ask_price, # Dealer parameter\n",
    "                              private_valuation_mean, private_valuation_sd, # Trader parameters\n",
    "                              alpha, beta, lower_q, upper_q, T, # Learning parameters\n",
    "                              k # Random seed for experiment\n",
    "                              ):\n",
    "    # Calculate initial information\n",
    "    possible_ask_price = np.arange(lowest_ask_price, highest_ask_price+1, 1)\n",
    "\n",
    "    # This is a nested list, the first layer is dealers, saves the greedy prices differences between period 1 and 2 in arrays for each experiment\n",
    "    historical_ask_price_difference = np.array([])\n",
    "    # This is a nested list, the first layer is period, saves the historical trading volume for each experiment\n",
    "    historical_trading_volume = [[np.array([])], [np.array([])]]\n",
    "\n",
    "    # Initate experiment with initial variables (Asset Payoff, Trader Valuation, Dealer Action, Q-Matrix)\n",
    "    asset_payoff = generate_asset_payoff(prob_low, prob_high, payoff_low, payoff_high,T,k)\n",
    "    trader_valuation_period1 = generate_trader_valuation(asset_payoff, private_valuation_mean,private_valuation_sd,k)\n",
    "    trader_valuation_period2 = generate_trader_valuation(asset_payoff, private_valuation_mean,private_valuation_sd,k+K)\n",
    "    dealer_action_period1 = [generate_dealer_action(beta,T,k+n*K) for n in np.arange(1,no_dealers+1,1)]\n",
    "    dealer_action_period2 = [generate_dealer_action(beta,T,k+2*n*K) for n in np.arange(1,no_dealers+1,1)]\n",
    "    Q_matrix = generate_discovery_q_matrix(lower_q,upper_q,len(possible_ask_price),no_dealers,k)\n",
    "\n",
    "    # Set random seed for ask_price randomisation during \"explore\", we iterate this over every experiment to make sure different experiments are selecting different \"explore\" ask prices\n",
    "    random.seed(k)\n",
    "\n",
    "    # Loop over each episode\n",
    "    for t in np.arange(0,T,1):\n",
    "        \n",
    "        \"\"\"\n",
    "        Period 1\n",
    "        \"\"\"\n",
    "        # We use an array to store ask prices of dealers\n",
    "        dealer_ask_prices_array_period1 = np.array([])\n",
    "\n",
    "        # Loop over all dealers and get ask price for each of them\n",
    "        for n in np.arange(no_dealers):\n",
    "\n",
    "            # Dealer n choose to explore or exploit\n",
    "            if dealer_action_period1[n][t] == \"explore\":\n",
    "                # If explore, ask_price is random integer from 1 to 15\n",
    "                dealer_ask_prices_array_period1 = np.append(dealer_ask_prices_array_period1, random.choice(possible_ask_price))\n",
    "\n",
    "            if dealer_action_period1[n][t] == \"exploit\":\n",
    "                # If exploit, ask_price is the action with the highest expected payoff from the dealer's Q_matrix at state null\n",
    "                dealer_ask_price = np.argmax(Q_matrix[n][0])+1\n",
    "                dealer_ask_prices_array_period1 = np.append(dealer_ask_prices_array_period1, dealer_ask_price)\n",
    "\n",
    "        # Minimum dealer price for this episode\n",
    "        lowest_ask_price_period1 = min(dealer_ask_prices_array_period1)\n",
    "\n",
    "        # Create an array that stores the state of the dealers in period 2\n",
    "        dealer_states = np.array([], dtype = int)\n",
    "        # Create an array that indicates the amount of trade each dealer got\n",
    "        dealer_trade_volume_period1 = np.array([], dtype = int)\n",
    "\n",
    "        # Informed trader now chooses whether to trade in period 1 of this episode according to the lowest_ask_price\n",
    "        if trader_valuation_period1[t] >= lowest_ask_price_period1: \n",
    "\n",
    "            # Save the historical trading volume in period 1\n",
    "            historical_trading_volume[0] = np.append(historical_trading_volume[0],1)\n",
    "\n",
    "            # Get the number of dealers to trade with in period 1\n",
    "            num_of_dealer_to_trade_period1 = list(dealer_ask_prices_array_period1).count(lowest_ask_price_period1)\n",
    "\n",
    "            # Now, we loop over the dealers to determine whether they gets the trade and how their Q-matrix is updated in period 1\n",
    "            for n in np.arange(no_dealers):\n",
    "\n",
    "                # Case 1) Dealer n gets the trade\n",
    "                if dealer_ask_prices_array_period1[n] == lowest_ask_price_period1:\n",
    "                    Q_matrix[n][0][int(lowest_ask_price_period1-1)] = alpha*(lowest_ask_price_period1*(1/num_of_dealer_to_trade_period1) + np.max(Q_matrix[n][no_dealers+3-num_of_dealer_to_trade_period1])) +\\\n",
    "                                                                      (1-alpha)*Q_matrix[n][0][int(lowest_ask_price_period1-1)]\n",
    "                    \n",
    "                    # Append to array that indicates dealer's state in the next period (this gives the index of the Q_matrix we should use for this dealer in the next period)\n",
    "                    dealer_states = np.append(dealer_states, no_dealers+3-num_of_dealer_to_trade_period1)\n",
    "                    # Append to array that indicates dealer's trade volume in period 1\n",
    "                    dealer_trade_volume_period1 = np.append(dealer_trade_volume_period1, 1/num_of_dealer_to_trade_period1)\n",
    "\n",
    "                # Case 2) Dealer n does not get the trade\n",
    "                else:\n",
    "                    #Q_matrix[n][0][int(dealer_ask_prices_array_period1[n]-1)] = alpha*(np.max(Q_matrix[n][2])) + (1-alpha)*Q_matrix[n][0][int(dealer_ask_prices_array_period1[n]-1)]\n",
    "\n",
    "                    # Append to array that indicates dealer's state (State = 0) in the next period\n",
    "                    dealer_states = np.append(dealer_states, 2)\n",
    "                    # Append to array that indicates dealer's trade volume in period 1\n",
    "                    dealer_trade_volume_period1 = np.append(dealer_trade_volume_period1, 0)\n",
    "            \n",
    "        # If no trade occurs in this episode\n",
    "        else:\n",
    "\n",
    "            # Save historical trading volume\n",
    "            historical_trading_volume[0] = np.append(historical_trading_volume[0],0)\n",
    "\n",
    "            # We update all of the dealers Q-matrix\n",
    "            for n in np.arange(no_dealers):\n",
    "                \n",
    "                # Since no trade occured, all dealers are updated in the same way\n",
    "                Q_matrix[n][0][int(dealer_ask_prices_array_period1[n]-1)] = alpha*(np.max(Q_matrix[n][1])) + (1-alpha)*Q_matrix[n][0][int(dealer_ask_prices_array_period1[n]-1)]\n",
    "\n",
    "                # Append to array that indicates dealer's state (State = NT) in the next period\n",
    "                dealer_states = np.append(dealer_states, 1)\n",
    "                # Append to array that indicates dealer's trade volume in period 1\n",
    "                dealer_trade_volume_period1 = np.append(dealer_trade_volume_period1, 0)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Period 2\n",
    "        \"\"\"\n",
    "        # We use an array to store ask prices of dealers\n",
    "        dealer_ask_prices_array_period2 = np.array([])\n",
    "\n",
    "        # Loop over all dealers and get ask price for each of them\n",
    "        for n in np.arange(no_dealers):\n",
    "\n",
    "            # Dealer n choose to explore or exploit\n",
    "            if dealer_action_period2[n][t] == \"explore\":\n",
    "                # If explore, ask_price is random integer from 1 to 15\n",
    "                dealer_ask_prices_array_period2 = np.append(dealer_ask_prices_array_period2, random.choice(possible_ask_price))\n",
    "\n",
    "            if dealer_action_period2[n][t] == \"exploit\":\n",
    "                # If exploit, ask_price is the action with the highest expected payoff from the dealer's Q_matrix at state dependent on the result of period 1\n",
    "                dealer_ask_price = np.argmax(Q_matrix[n][dealer_states[n]])+1\n",
    "                dealer_ask_prices_array_period2 = np.append(dealer_ask_prices_array_period2, dealer_ask_price)\n",
    "\n",
    "                # Save greedy price if dealer plays exploit for both rounds\n",
    "                #if dealer_action_period1[n][t] == \"exploit\":\n",
    "                #    historical_greedy_price_difference[n] = np.append(historical_greedy_price_difference[n], dealer_ask_prices_array_period2[n] - dealer_ask_prices_array_period1[n])\n",
    "\n",
    "        # Minimum dealer price for this episode\n",
    "        lowest_ask_price_period2 = min(dealer_ask_prices_array_period2)\n",
    "        # Save differences in best ask price\n",
    "        historical_ask_price_difference = np.append(historical_ask_price_difference, lowest_ask_price_period2-lowest_ask_price_period1)\n",
    "\n",
    "        # Informed trader now chooses whether to trade in period 2 of this episode according to the lowest_ask_price\n",
    "        if trader_valuation_period2[t] >= lowest_ask_price_period2: \n",
    "\n",
    "            # Save the historical trading volume in period 2\n",
    "            historical_trading_volume[1] = np.append(historical_trading_volume[1],1)\n",
    "\n",
    "            # Get the number of dealers to trade with in period 2\n",
    "            num_of_dealer_to_trade_period2 = list(dealer_ask_prices_array_period2).count(lowest_ask_price_period2)\n",
    "\n",
    "            # Now, we loop over the dealers to determine whether they gets the trade and how their Q-matrix is updated in period 2\n",
    "            for n in np.arange(no_dealers):\n",
    "\n",
    "                # Case 1) Dealer n gets the trade in period 2\n",
    "                if dealer_ask_prices_array_period2[n] == lowest_ask_price_period2:\n",
    "                    Q_matrix[n][dealer_states[n]][int(lowest_ask_price_period2-1)] = alpha*(lowest_ask_price_period2*(1/num_of_dealer_to_trade_period2) -\\\n",
    "                                                                                            asset_payoff[t]*(dealer_trade_volume_period1[n] + 1/num_of_dealer_to_trade_period2)) +\\\n",
    "                                                                                    (1-alpha)*Q_matrix[n][dealer_states[n]][int(lowest_ask_price_period2-1)]\n",
    "                    \n",
    "                # Case 2) Dealer n does not get the trade in period 2\n",
    "                else:\n",
    "                    Q_matrix[n][dealer_states[n]][int(dealer_ask_prices_array_period2[n]-1)] = alpha*(-asset_payoff[t]*(dealer_trade_volume_period1[n])) +\\\n",
    "                                                                                               (1-alpha)*Q_matrix[n][dealer_states[n]][int(dealer_ask_prices_array_period2[n]-1)]\n",
    "            \n",
    "        # If no trade occurs in this period 2 of this episode\n",
    "        else:\n",
    "\n",
    "            # Save historical trading volume in period 2\n",
    "            historical_trading_volume[1] = np.append(historical_trading_volume[1],0)\n",
    "\n",
    "            # We update all of the dealers Q-matrix\n",
    "            for n in np.arange(no_dealers):\n",
    "                \n",
    "                # Since no trade occured, all dealers are updated in the same way\n",
    "                Q_matrix[n][dealer_states[n]][int(dealer_ask_prices_array_period2[n]-1)] = alpha*(-asset_payoff[t]*(dealer_trade_volume_period1[n])) +\\\n",
    "                                                                                           (1-alpha)*Q_matrix[n][dealer_states[n]][int(dealer_ask_prices_array_period2[n]-1)]\n",
    "    \n",
    "    \n",
    "    # For tracking of progress, prints every 10 iteration of the experiment\n",
    "    if k%10 == 0: print(f\" Processor{current_process().name} is processing k={k}\")\n",
    "\n",
    "    # Final calculations\n",
    "    price_discovery = historical_ask_price_difference[historical_trading_volume[0] == 1].mean() - historical_ask_price_difference[historical_trading_volume[0] == 0].mean()\n",
    "    best_ask_difference = np.mean(historical_ask_price_difference)\n",
    "\n",
    "    # Return historical_greedy_price and mean trading volume\n",
    "    return price_discovery, best_ask_difference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
